{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-22T14:20:20.824064Z","iopub.status.busy":"2022-09-22T14:20:20.823617Z","iopub.status.idle":"2022-09-22T14:20:31.746439Z","shell.execute_reply":"2022-09-22T14:20:31.744748Z","shell.execute_reply.started":"2022-09-22T14:20:20.823953Z"},"trusted":true},"outputs":[],"source":["# Install gym\n","!pip install gym"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-22T14:20:31.749862Z","iopub.status.busy":"2022-09-22T14:20:31.749258Z","iopub.status.idle":"2022-09-22T14:20:31.878973Z","shell.execute_reply":"2022-09-22T14:20:31.87805Z","shell.execute_reply.started":"2022-09-22T14:20:31.749808Z"},"trusted":true},"outputs":[],"source":["# use the library 'gym' for this environment\n","import gym\n","import numpy as np\n","\n","env = gym.make('CartPole-v1')\n","env.action_space\n","env.reset()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-22T14:20:31.88077Z","iopub.status.busy":"2022-09-22T14:20:31.880165Z","iopub.status.idle":"2022-09-22T14:20:31.891007Z","shell.execute_reply":"2022-09-22T14:20:31.889755Z","shell.execute_reply.started":"2022-09-22T14:20:31.880735Z"},"trusted":true},"outputs":[],"source":["# Q table implementation \n","state_space = 4  # number of state variables\n","action_space = 2  # number of actions\n","\n","def Qtable(state_space, action_space, bin_size=30):\n","    bins = [np.linspace(-4.8, 4.8, bin_size),  # cart position --> limits are taken from env page\n","            np.linspace(-4, 4, bin_size),  # cart velocity\n","            np.linspace(-0.418, 0.418, bin_size),  # pole angle (radians)\n","            np.linspace(-4, 4, bin_size)]  # pole angular velocity\n","\n","    q_table = np.random.uniform(low=-1, high=1, size=([bin_size] * state_space + [action_space]))\n","    return q_table, bins\n","\n","def Discrete(state, bins, is_env_reset=False):\n","    \"\"\"Discretizes the given continuous input state observation.\"\"\"\n","    index = []\n","    main = state\n","    if is_env_reset: main = state[0]\n","    for i in range(len(main)): \n","        index.append(np.digitize(main[i], bins[i]) - 1)\n","    return tuple(index)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-22T14:20:31.894427Z","iopub.status.busy":"2022-09-22T14:20:31.894028Z","iopub.status.idle":"2022-09-22T14:20:31.907885Z","shell.execute_reply":"2022-09-22T14:20:31.906711Z","shell.execute_reply.started":"2022-09-22T14:20:31.89439Z"},"trusted":true},"outputs":[],"source":["# Q learning function takes the Q table as input\n","def Q_learning(q_table, bins, episodes=5000, gamma=0.95, lr=0.1, timestep=100, epsilon=0.2):\n","    steps = 0\n","    curr_score_history = []  # Tracks episode level scores\n","    for episode_i in range(1, episodes + 1):\n","        steps += 1\n","        current_state = Discrete(env.reset(), bins, True)\n","        current_score = 0\n","        done = False\n","        while not done:\n","            if np.random.uniform(0, 1) < epsilon:  # epsilon-greedy approach\n","                action = env.action_space.sample()\n","            else:\n","                action = np.argmax(q_table[current_state])\n","            observation, reward, done, info, empty = env.step(action)\n","            next_state = Discrete(observation, bins)\n","            current_score += reward  # accumulate current_score until finish\n","\n","            if not done:\n","                max_future_q = np.max(q_table[next_state])\n","                current_q = q_table[current_state + (action,)]\n","                new_q = (1 - lr) * current_q + lr * (reward + gamma * max_future_q)\n","                q_table[current_state + (action,)] = new_q\n","            else:\n","                curr_score_history.append(current_score)\n","            \n","            current_state = next_state\n","        \n","        avg_score = np.mean(curr_score_history[-25:])  # average score of last 5 episodes\n","        if episode_i % timestep == 0: print('Average score after {} episodes:- {}'.format(episode_i, avg_score))\n","        if avg_score >= 150:  # success score is subjective\n","            print('Problem solved in episode {} with steps {}'.format(episode_i, steps))\n","            return curr_score_history\n","    return curr_score_history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-22T14:20:31.910461Z","iopub.status.busy":"2022-09-22T14:20:31.909293Z","iopub.status.idle":"2022-09-22T14:20:35.550956Z","shell.execute_reply":"2022-09-22T14:20:35.549649Z","shell.execute_reply.started":"2022-09-22T14:20:31.910414Z"},"trusted":true},"outputs":[],"source":["# Q-learning\n","q_table, bins = Qtable(4, 2)\n","score_history = Q_learning(q_table, bins, episodes=2000, gamma=0.995, lr=0.15)\n","env.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-22T14:20:35.552808Z","iopub.status.busy":"2022-09-22T14:20:35.5524Z","iopub.status.idle":"2022-09-22T14:20:35.777299Z","shell.execute_reply":"2022-09-22T14:20:35.776099Z","shell.execute_reply.started":"2022-09-22T14:20:35.552772Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# plot score vs no.of episodes\n","plt.plot(score_history)\n","plt.xlabel('No of episodes')\n","plt.ylabel('Reward')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
